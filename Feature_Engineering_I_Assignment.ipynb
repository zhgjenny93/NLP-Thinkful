{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering I-Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIkaHcKSb+K+84nxNrkk+U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhgjenny93/NLP-Thinkful/blob/main/Feature_Engineering_I_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkAdVvzaD71H"
      },
      "source": [
        "1. Your task is to increase the performance of the models that you implemented in the bank-of-words example. Here are some suggested avenues of investigation:\n",
        "\n",
        "  1. Other modeling techniques and models\n",
        "\n",
        "  2. Making more features that take advantage of the spaCy information, such as grammar, phrases, parts of speech, and so forth\n",
        "\n",
        "  3. Making sentence-level features, such as the number of words and amount of punctuation\n",
        "\n",
        "  4. Including contextual information, such as the length of previous and next sentences, words repeated from one sentence to the next, and so on\n",
        "\n",
        "  5. Or anything else that your heart desires\n",
        "\n",
        "    Compare your models' performances with those of the example.\n",
        "\n",
        "2. In the 2-gram example above, you only used 2-gram as your features. This time, use both 1-gram and 2-gram features together as your feature set. Run the same models as in the example and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZBj5UdfACtQ",
        "outputId": "804fece9-9086-425a-e1d8-0000c3006804"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De1fIevuFb5k"
      },
      "source": [
        "# Utility function for standard text cleaning\n",
        "def text_cleaner(text):\n",
        "  # Visual inspection identifies a form of punctuation spaCy does not \n",
        "  # recognize:the double dash '--'.\n",
        "  text = re.sub(r'--', ' ', text)\n",
        "  text = re.sub('[\\[].*?[\\]]', '', text)\n",
        "  text = re.sub(r'(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b', ' ', text)\n",
        "  text = ' '.join(text.split())\n",
        "\n",
        "  return text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umCcILVQGDbK"
      },
      "source": [
        "# load and clean the data\n",
        "persuasion =gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# clean chapter indicators\n",
        "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "\n",
        "alice = text_cleaner(alice)\n",
        "persuasion = text_cleaner(persuasion)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYFYX91gGfaL"
      },
      "source": [
        "# Parse the cleaned novels\n",
        "nlp = spacy.load('en')\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "GYq1tKXvGoOC",
        "outputId": "b26e5bdd-53bd-45ce-d49e-01ed3612f9f8"
      },
      "source": [
        "# group into sentences\n",
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "\n",
        "# combine the sentences from the two novels into one data frame\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = ['text', 'author'])\n",
        "sentences.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(I, shall, be, late, !, ')</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text   author\n",
              "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
              "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "3                                      (Oh, dear, !)  Carroll\n",
              "4                         (I, shall, be, late, !, ')  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gskJVIX4HE6u"
      },
      "source": [
        "# Remove stop words and punctuation and lemmatize tokens\n",
        "for i, sentence in enumerate(sentences['text']):\n",
        "  sentences.loc[i, \"text\"] = ' '.join(\n",
        "      [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yhzVY4RHbCl"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='word')\n",
        "X = vectorizer.fit_transform(sentences['text'])\n",
        "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
        "sentences = pd.concat([bow_df, sentences[['text', 'author']]], axis=1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ediAX_PIdhT",
        "outputId": "2819351f-a260-44be-d047-e65185da6c8e"
      },
      "source": [
        "# Tune hyperparamters with GridSearchCV for best model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text', 'author'], 1))\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr_params = {\"penalty\": ['l1', 'l2']}\n",
        "lr = LogisticRegression()\n",
        "\n",
        "rfc_params = {'n_estimators': [3, 5, 10, 15],\n",
        "              'max_depth':[2, 3, 4, 5],\n",
        "              'min_samples_split': [3, 5, 7, 9]}\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "gbc_params = {'n_estimators': [3, 5, 10, 15],\n",
        "              'max_depth': [2, 3, 4, 5],\n",
        "              'min_samples_split': [3, 5, 7, 9]}\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "clf_lr = GridSearchCV(lr, lr_params, cv=5)\n",
        "clf_lr.fit(X_train, y_train)\n",
        "\n",
        "clf_rfc = GridSearchCV(rfc, rfc_params, cv=5)\n",
        "clf_rfc.fit(X_train, y_train)\n",
        "\n",
        "clf_gbc = GridSearchCV(gbc, gbc_params, cv=5)\n",
        "clf_gbc.fit(X_train, y_train)\n",
        "\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', clf_lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', clf_lr.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', clf_rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', clf_rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', clf_gbc.score(X_train, y_train))\n",
        "print('\\nTest set score:', clf_gbc.score(X_test, y_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.9266780675502352\n",
            "\n",
            "Test set score: 0.8572649572649572\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.7640017101325353\n",
            "\n",
            "Test set score: 0.7598290598290598\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.7785378366823429\n",
            "\n",
            "Test set score: 0.782051282051282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrgqcqrqSWFr"
      },
      "source": [
        "Logistic regression model did not perform any better, and It looks like it is still overfit. The random forest model no longer appears to be overfitted, however, the accuracy of the model went down. Similarly, the gradient boosting classifier model is not overfitted, but the accuracy decreased a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "-5IFTrJWThSK",
        "outputId": "ef8cdae1-ba98-404a-ea8f-0c56736e40cb"
      },
      "source": [
        "# Using both 1 and 2-grams features\n",
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(sentences['text'])\n",
        "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
        "sentences = pd.concat([bow_df, sentences[['text', 'author']]], axis=1)\n",
        "sentences.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1st</th>\n",
              "      <th>29th</th>\n",
              "      <th>29th september</th>\n",
              "      <th>abbreviation</th>\n",
              "      <th>abbreviation living</th>\n",
              "      <th>abdication</th>\n",
              "      <th>abdication neighbour</th>\n",
              "      <th>abide</th>\n",
              "      <th>abide consequence</th>\n",
              "      <th>abide figure</th>\n",
              "      <th>ability</th>\n",
              "      <th>ability affection</th>\n",
              "      <th>ability awkwardness</th>\n",
              "      <th>ability difficulty</th>\n",
              "      <th>able</th>\n",
              "      <th>able attempt</th>\n",
              "      <th>able avail</th>\n",
              "      <th>able avoid</th>\n",
              "      <th>able bear</th>\n",
              "      <th>able convince</th>\n",
              "      <th>able devise</th>\n",
              "      <th>able eat</th>\n",
              "      <th>able far</th>\n",
              "      <th>able feign</th>\n",
              "      <th>able join</th>\n",
              "      <th>able judge</th>\n",
              "      <th>able leave</th>\n",
              "      <th>able letter</th>\n",
              "      <th>able live</th>\n",
              "      <th>able marry</th>\n",
              "      <th>able persuade</th>\n",
              "      <th>able regard</th>\n",
              "      <th>able remain</th>\n",
              "      <th>able return</th>\n",
              "      <th>able ring</th>\n",
              "      <th>able rise</th>\n",
              "      <th>able set</th>\n",
              "      <th>able shew</th>\n",
              "      <th>able speak</th>\n",
              "      <th>able tell</th>\n",
              "      <th>...</th>\n",
              "      <th>young young</th>\n",
              "      <th>younker</th>\n",
              "      <th>youth</th>\n",
              "      <th>youth beauty</th>\n",
              "      <th>youth bloom</th>\n",
              "      <th>youth early</th>\n",
              "      <th>youth father</th>\n",
              "      <th>youth fine</th>\n",
              "      <th>youth hardly</th>\n",
              "      <th>youth hope</th>\n",
              "      <th>youth jaw</th>\n",
              "      <th>youth kill</th>\n",
              "      <th>youth learn</th>\n",
              "      <th>youth like</th>\n",
              "      <th>youth mention</th>\n",
              "      <th>youth possibly</th>\n",
              "      <th>youth restore</th>\n",
              "      <th>youth say</th>\n",
              "      <th>youth spring</th>\n",
              "      <th>youth value</th>\n",
              "      <th>youth vigour</th>\n",
              "      <th>youthful</th>\n",
              "      <th>youthful infatuation</th>\n",
              "      <th>zeal</th>\n",
              "      <th>zeal business</th>\n",
              "      <th>zeal common</th>\n",
              "      <th>zeal dwell</th>\n",
              "      <th>zeal sport</th>\n",
              "      <th>zeal think</th>\n",
              "      <th>zealand</th>\n",
              "      <th>zealand australia</th>\n",
              "      <th>zealous</th>\n",
              "      <th>zealous officer</th>\n",
              "      <th>zealous subject</th>\n",
              "      <th>zealously</th>\n",
              "      <th>zealously discharge</th>\n",
              "      <th>zigzag</th>\n",
              "      <th>zigzag go</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>remarkable Alice think way hear Rabbit oh dear</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>oh dear</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>shall late</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 35313 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   1st  29th  ...                                               text   author\n",
              "0    0     0  ...  Alice begin tired sit sister bank have twice p...  Carroll\n",
              "1    0     0  ...  consider mind hot day feel sleepy stupid pleas...  Carroll\n",
              "2    0     0  ...     remarkable Alice think way hear Rabbit oh dear  Carroll\n",
              "3    0     0  ...                                            oh dear  Carroll\n",
              "4    0     0  ...                                         shall late  Carroll\n",
              "\n",
              "[5 rows x 35313 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DF5bj4PaWGr",
        "outputId": "b295cabd-ed5f-4400-a409-56a77fb50310"
      },
      "source": [
        "Y = sentences['author']\n",
        "X = np.array(sentences.drop(['text', 'author'], 1))\n",
        "\n",
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Models\n",
        "lr = LogisticRegression()\n",
        "rfc = RandomForestClassifier()\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rfc.fit(X_train, y_train)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "print(\"----------------------Logistic Regression Scores----------------------\")\n",
        "print('Training set score:', lr.score(X_train, y_train))\n",
        "print('\\nTest set score:', lr.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Random Forest Scores----------------------\")\n",
        "print('Training set score:', rfc.score(X_train, y_train))\n",
        "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
        "\n",
        "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
        "print('Training set score:', gbc.score(X_train, y_train))\n",
        "print('\\nTest set score:', gbc.score(X_test, y_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------Logistic Regression Scores----------------------\n",
            "Training set score: 0.9450619923044036\n",
            "\n",
            "Test set score: 0.8589743589743589\n",
            "----------------------Random Forest Scores----------------------\n",
            "Training set score: 0.9687900812312954\n",
            "\n",
            "Test set score: 0.8376068376068376\n",
            "----------------------Gradient Boosting Scores----------------------\n",
            "Training set score: 0.8191534843950407\n",
            "\n",
            "Test set score: 0.8128205128205128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr2bLifG3fGn"
      },
      "source": [
        "Running 1-gram and 2-grams combined in the models contributed to better performance overall."
      ]
    }
  ]
}